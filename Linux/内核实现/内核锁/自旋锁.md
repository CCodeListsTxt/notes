## 原始自旋锁

原始自旋锁中所有等待同一个自旋锁的 cpu 在同一个变量上自选等待，获得或者释放锁时会对该变量进行修改。

为了保证缓存一致性，一个 cpu 修改变量后必须让其他 cpu 上对应变量的缓存行失效，因此每次获取或释放锁都会让等待同一锁的 cpu 刷新对应变量的缓存，在大型系统中会导致性能大幅度下降。

## ticket lock

```cpp
struct ticket_spinlock{
    uint16_t next;
    uint16_t owner;
};
```

ticket lock 的基本逻辑: 获取锁时，原子增加 `next` 的值，然后作为自己的 ticket。释放锁时，原子增加 `owner` 的值。

ticket lock 解决了原始自旋锁中，请求锁的顺序和获取锁的顺序不一致的问题，但没有解决缓存问题。

## mcs lock

为了解决上述问题，内核设计设设计出了 msc 自旋锁。msc 自旋锁的主要策略是为每一个 cpu 创建一个副本变量，在本地变量中自旋，避免缓存同步的开销。

> mcs 的命名为 'Mellor-Crummey' 和 'Scott' 两个发明人的首字母缩写。

```cpp
struct mcs_spinlock {
	struct mcs_spinlock *next;
	int locked;
	int count;
};
```

- `next`: 下一个请求获取锁的。

- `locked`: 是否获得锁。

- `count`: 当前节点自旋的次数。

当持有自旋锁的 cpu 释放时，会让其 `next` 节点的 `locked` 域置为 1，从而让对应的 cpu 跳出自旋。

#### 简易实现

```cpp
#include <atomic>
#include <latch>
#include <print>
#include <thread>

struct mcs_node_t {
    mcs_node_t *next;
    bool locked;
};

class mcs_lock_t {
  public:
    void lock(mcs_node_t &node) {
        node.next = nullptr;
        node.locked = true;

        // 将当前节点添加到等待队列尾部，并获取前驱节点
        mcs_node_t *prev = tail.exchange(&node);
        if (prev != nullptr) {
            // 如果有前驱节点，将当前节点链接到前驱节点后
            prev->next = &node;
            // 自旋等待直到获得锁
            while (node.locked) {
                std::this_thread::yield();
            }
        }
    }

    void unlock(mcs_node_t &node) {
        if (node.next == nullptr) {
            mcs_node_t *expected = &node;
            if (tail.compare_exchange_strong(expected, nullptr)) {
                // 如果当前没有正在链接的节点，直接返回
                return;
            }
            // node.next == null && tail != node,
            // 即存在节点正在链接, 等待其链接完成
            while (node.next == nullptr) {
                std::this_thread::yield();
            }
        }
        // 通知后继节点可以获取锁
        node.next->locked = false;
    }

  private:
    std::atomic<mcs_node_t *> tail{nullptr};
};

auto val = 0;
auto thread_count = 10;
auto inc_times = 1000000;
auto latch = std::latch{thread_count};
auto lock = mcs_lock_t{};

auto main() -> int {
    for (auto i = 0; i < thread_count; ++i) {
        std::thread{[] {
            auto node = mcs_node_t{};
            for (auto i = 0; i < inc_times; ++i) {
                lock.lock(node);
                val += 1;
                lock.unlock(node);
            }
            latch.count_down();
        }}.detach();
    }
    latch.wait();
    std::println("{}", thread_count * inc_times == val); // true

    return 0;
}
```

## 队列自旋锁

在 64 位系统中，仅 mcs lock 自身就需要占据 16 字节大小，而内核中许多数据结构都需要内嵌自旋锁作为成员，为了解决这个问题，linux 内核现在使用队列自旋锁。

#### 自旋锁结构

```cpp
// include/asm-generic/qspinlock_types.h

struct qspinlock {
    union {
        atomic_t val;

        struct {
            uint8_t locked;
            uint8_t pending;
        };

        struct {
            uint16_t locked_pending;
            uint16_t tail;
        };
    };
};
```

- `locked`: 表示锁是否被 cpu 持有。

- `pending`: 表示是否有 cpu 正在尝试获取锁。

- `tail`: 通过该域可以找到自选队列的最后一个节点位置。

  - 2bit 表示在 cpu 上节点的序号。

    > - 尝试获取自旋锁时会禁止内核抢占，所以进程在等待自旋锁时不会被其他进程抢占。自旋锁不可重入，一个自旋锁只能被同一个 cpu 获取一次。
    >
    > - cpu 在等待一个自旋锁时可能被软中断抢占，然后在处理程序中等待另一个自旋锁。
    >
    > - 软中断处理程序过程中可能被硬中断抢占，然后在处理程序中等待另一个自旋锁。
    >
    > - 硬中断处理程序过程中可能被不可屏蔽终端抢占，然后在处理程序中等待另一个自旋锁。
    >
    > 因此每个 cpu 上最多只能同时持有 4 个自旋锁：进程上下文自旋锁、软中断上下文自旋锁、硬中断上下文自旋锁和不可屏蔽中断自旋锁。
    >
    > 如果在某些架构下，不可屏蔽中断还可被其他中断抢占，如果此时锁的嵌套深度大于 4，则更深层次时直接在锁上自旋。

  - 14bit 表示在哪个 cpu 上等待该自旋锁。

#### 自旋锁节点

队列自旋锁可以通过 `tail` 域定位到唯一的锁节点，因此优化掉了 msc lock 需要的指针。

```cpp
// kernel/locking/qspinlock.c

#define MAX_NODES 4

struct qnode {
    struct mcs_spinlock mcs;
};

// 按缓存行大小对齐，为每个 cpu 创建一个 4 大小的节点数组
static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]);
```

#### 内核实现

因为用户态无法禁止内核调度，如果需要实现内核态的队列自旋锁，无法将其大小压缩至 4 字节。因此在用户态中实现 qspinlock 无意义。

```cpp
// include/asm-generic/qspinlock.h

static __always_inline void queued_spin_lock(struct qspinlock *lock) {
    int val = 0;

    // 如果此时 lock->val 为 0，则表示锁空闲，可以立即获取锁
    // 否则，表示锁已经被其他线程占用，需要进行慢速路径处理
    if (likely(atomic_try_cmpxchg_acquire(&lock->val, &val, _Q_LOCKED_VAL)))
        return;

    // val == lock->val
    queued_spin_lock_slowpath(lock, val);
}
```

```cpp
// include/asm-generic/qspinlock.h

// 进入 queued_spin_lock 时已经关闭内核抢占
static __always_inline void queued_spin_lock(struct qspinlock *lock)
{
	int val = 0;

	if (likely(atomic_try_cmpxchg_acquire(&lock->val, &val, _Q_LOCKED_VAL)))
		return;

	queued_spin_lock_slowpath(lock, val);
}


// kernel/locking/qspinlock.c

// 获取 locked 状态的 cpu 持有锁
// 获取 pending 状态的 cpu 等待锁释放
// 其余 cpu 进入队列自旋
void __lockfunc queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
{
	struct mcs_spinlock *prev, *next, *node;
	u32 old, tail;
	int idx;

	// 确保CPU数量不会超过tail字段的位数
	BUILD_BUG_ON(CONFIG_NR_CPUS >= (1U << _Q_TAIL_CPU_BITS));

	// 如果启用了准虚拟化，直接进入队列模式
	if (pv_enabled())
		goto pv_queue;

	// 虚拟化环境下的特殊处理
	if (virt_spin_lock(lock))
		return;

	// 如果调用时锁的状态为 (0,1,0)
	// 即只有一个 cpu 正在尝试获取锁，但还未完成
	// 则自旋直到到达最大尝试次数，或者锁状态变化
	// 通过该优化，可以避免不必要的队列操作
	if (val == _Q_PENDING_VAL) {
		int cnt = _Q_PENDING_LOOPS;
		val = atomic_cond_read_relaxed(
			&lock->val, (VAL != _Q_PENDING_VAL) || !cnt--);
	}

	// 检查除 locked 之外是否还有其他设置
	// 如果有，则表示当前存在竞争，进入队列模式
	if (val & ~_Q_LOCKED_MASK)
		goto queue;

	// 如果不存在竞争关系，尝试获取 pending
	val = queued_fetch_set_pending_acquire(lock);

	// 检测是否获取到 pending
	// 如果获取失败，则存在竞争关系，恢复 pending 状态并进入队列模式
	if (unlikely(val & ~_Q_LOCKED_MASK)) {
		// 如果当前锁状态不是 pending，则清除 pending 状态
		if (!(val & _Q_PENDING_MASK))
			clear_pending(lock);
		goto queue;
	}

	// 获取 pending 成功后，等待持有者释放锁
	if (val & _Q_LOCKED_MASK)
		smp_cond_load_acquire(&lock->locked, !VAL);

	// 获取锁并释放 pending 状态
	clear_pending_set_locked(lock);
	lockevent_inc(lock_pending);
	return;

	// 进入 mcs 队列模式
queue:
	lockevent_inc(lock_slowpath); // 记录慢速路径调用次数
pv_queue:
	node = this_cpu_ptr(&qnodes[0].mcs);
	idx = node->count++; // qnodes[0].count 表示当前 cpu 上自旋锁的嵌套深度
	tail = encode_tail(smp_processor_id(), idx); // 计算当前 cpu 的 tail

	// 跟踪锁竞争的调试工具
	trace_contention_begin(lock, LCB_F_SPIN);

	// 如果锁的嵌套深度大于最大节点数量，直接自旋
	if (unlikely(idx >= MAX_NODES)) {
		lockevent_inc(lock_no_node);
		while (!queued_spin_trylock(lock))
			cpu_relax();
		goto release;
	}

	// 获取实际的 mcs 节点
	node = grab_mcs_node(node, idx);

	// 记录非零索引值的使用计数
	lockevent_cond_inc(lock_use_node2 + idx - 1, idx);

	// 防止编译指令重排
	barrier();

	// 初始化MCS节点
	node->locked = 0;
	node->next = NULL;
	pv_init_node(node);

	// 访问 node 时进行了冷缓存
	// 期间锁可能已经被释放
	// 因此尝试获取一次锁
	if (queued_spin_trylock(lock))
		goto release;

	// 保证 node 已初始化完成
	smp_wmb();

	// 更新 tail，并获取前驱 tail
	old = xchg_tail(lock, tail);
	next = NULL;

	/*
	 * 如果队列非空:
	 * 1. 将当前节点链接到前驱节点
	 * 2. 等待前驱节点通知
	 */
	if (old & _Q_TAIL_MASK) {
		// 加入队列
		prev = decode_tail(old);
		WRITE_ONCE(prev->next, node);

		pv_wait_node(node, prev);
		arch_mcs_spin_lock_contended(&node->locked);

		/*
		 * 预取可能的后继节点以优化后续unlock
		 */
		next = READ_ONCE(node->next);
		if (next)
			prefetchw(next);
	}

	/*
	 * 等待锁释放
	 * 使用acquire语义以配合unlock的release语义
	 */
	if ((val = pv_wait_head_or_lock(lock, node)))
		goto locked;

	val = atomic_cond_read_acquire(&lock->val,
				       !(VAL & _Q_LOCKED_PENDING_MASK));

locked:
	/*
	 * 获取锁:
	 * 如果是队列中唯一节点且无pending,清除tail并获取锁
	 * 否则仅获取锁
	 */
	if ((val & _Q_TAIL_MASK) == tail) {
		if (atomic_try_cmpxchg_relaxed(&lock->val, &val, _Q_LOCKED_VAL))
			goto release;
	}

	/* 设置locked位获取锁 */
	set_locked(lock);

	/*
	 * 等待后继节点(如果还未看到)并通知它
	 */
	if (!next)
		next = smp_cond_load_relaxed(&node->next, (VAL));

	arch_mcs_spin_unlock_contended(&next->locked);
	pv_kick_node(lock, next);

release:
	trace_contention_end(lock, 0);

	/* 释放MCS节点 */
	__this_cpu_dec(qnodes[0].mcs.count);
}
```
